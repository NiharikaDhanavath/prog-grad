{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "edqIcSwhd1k1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8pSTazYDYRrG"
      },
      "outputs": [],
      "source": [
        "class SearchEngine:\n",
        "\n",
        "  def __init__(self, path = None):\n",
        "    self.readDocs(path)\n",
        "    self.buildInvertedIndex()\n",
        "\n",
        "  def cleanToken(self, token: str):\n",
        "    puncs = string.punctuation+string.whitespace+'\\xa0'\n",
        "    token = ''.join(list(filter(lambda x: (x.isalnum() or x in puncs), list(token))))\n",
        "    token = token.strip(puncs).lower()\n",
        "    return token\n",
        "\n",
        "\n",
        "  def readDocs(self,path):\n",
        "\n",
        "    self.mapper = dict() #maps urls to the set of unique words\n",
        "\n",
        "    try:\n",
        "      with open(path, 'r') as file:\n",
        "        data = file.readlines()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "      print(f\"the file - {path} is not present\")\n",
        "      return None\n",
        "\n",
        "    data = list(filter(lambda x: x != '\\n', data)) # removing empty lines\n",
        "    data = [i.strip('\\n') for i in data]\n",
        "\n",
        "    prevline = '<endPageBody>'\n",
        "    page_content = []\n",
        "    cur_url = None\n",
        "\n",
        "    print('Stand by while building index...')\n",
        "    for line in tqdm(data):\n",
        "\n",
        "      if line.startswith('http://') or line.startswith('https://'):\n",
        "        if prevline=='<endPageBody>':\n",
        "          page_content = page_content[1:-2] # to remove <pageBody> and <endPageBody>\n",
        "          tokens = list(set(' '.join(page_content).split(' ')))\n",
        "          tokens = set([self.cleanToken(i) for i in tokens])\n",
        "          self.mapper[cur_url] = tokens\n",
        "          cur_url = line\n",
        "          page_content = []\n",
        "      else:\n",
        "        page_content.append(line)\n",
        "      prevline = line\n",
        "\n",
        "    if page_content and cur_url:\n",
        "      page_content = page_content[1:-2] # to remove <pageBody> and <endPageBody>\n",
        "      tokens = list(set(' '.join(page_content).split(' ')))\n",
        "      tokens = set([self.cleanToken(i) for i in tokens])\n",
        "      self.mapper[cur_url] = tokens\n",
        "\n",
        "    del self.mapper[None]\n",
        "\n",
        "    unique_words = set()\n",
        "    for i in self.mapper.values(): unique_words|=i\n",
        "\n",
        "    print(f\"Indexed {len(self.mapper.keys())} pages containing {len(unique_words)} unique terms\")\n",
        "\n",
        "  def buildInvertedIndex(self):\n",
        "\n",
        "    self.inverted_index = dict()\n",
        "\n",
        "    urls = list(self.mapper.keys())\n",
        "    words = set()\n",
        "    for value in self.mapper.values():\n",
        "      words |= value\n",
        "\n",
        "    for word in tqdm(words):\n",
        "      self.inverted_index[word] = set()\n",
        "\n",
        "      for url in urls:\n",
        "        if word in self.mapper[url]:\n",
        "          self.inverted_index[word].add(url)\n",
        "\n",
        "\n",
        "  def findQueryMatches(self, query):\n",
        "\n",
        "    words = query.lower().split(' ')\n",
        "    result = set()\n",
        "\n",
        "    for word in words:\n",
        "      prefix = word[0]\n",
        "      if prefix == '+':\n",
        "        result &= self.inverted_index.get(word[1:],set())\n",
        "      elif prefix == '-':\n",
        "        result -= self.inverted_index.get(word[1:],set())\n",
        "      else:\n",
        "        result |= self.inverted_index.get(word,set())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPGR6jRRcj6X",
        "outputId": "7663de6e-22dd-4e19-8c07-c7627f20a2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stand by while building index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 361/361 [00:00<00:00, 23924.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed 5 pages containing 1460 unique terms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1460/1460 [00:00<00:00, 246863.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a query to run on the file (enter RETURN/ENTER to exit): gupta\n",
            "There are 2 matches for the query - \"gupta\"\n",
            "https://www.cs.wmich.edu/gupta/teaching/cs5950/5950F23PGSweb/TopicsCovered%20ProgGradStu.html\n",
            "https://www.cs.wmich.edu/~gupta/teaching/cs603/wsnSp04/ClassPolicies.html\n",
            "Enter a query to run on the file (enter RETURN/ENTER to exit): add +in\n",
            "There are 2 matches for the query - \"add +in\"\n",
            "https://www.cs.wmich.edu/gupta/teaching/cs5950/5950F23PGSweb/TopicsCovered%20ProgGradStu.html\n",
            "https://cs.wmich.edu/elise/courses/cs531/assignments-SI19.html\n",
            "Enter a query to run on the file (enter RETURN/ENTER to exit): \n",
            "program terminated successfully!!\n"
          ]
        }
      ],
      "source": [
        "mysearchengine = SearchEngine('/content/sampleWebsiteData.txt')\n",
        "while True:\n",
        "  query = input(\"Enter a query to run on the file (enter RETURN/ENTER to exit): \").strip()\n",
        "  if query == \"\": break\n",
        "  result = mysearchengine.findQueryMatches(query)\n",
        "  n = len(result)\n",
        "  if n>0:\n",
        "    print(f\"There are {n} matches for the query - \\\"{query}\\\"\")\n",
        "    for url in result: print(url)\n",
        "  else:\n",
        "    print(f\"There are no matches for the given query - \\\"{query}\\\"\")\n",
        "print('program terminated successfully!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xStP6Z5NiTw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}